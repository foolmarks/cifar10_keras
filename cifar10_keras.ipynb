{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue;\">CIFAR-10 example using Keras</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>This notebook will show you how to create, train and evaluate a small convolution network to work on the CIFAR-10 dataset.\n",
    "\n",
    "The first thing we will do is the usual housekeeping ..import the required python libraries and set up directories. There's no machine learning specific code required here, just plain vanilla Python (..I use Python3)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "TB_LOG_DIR = Path('tb_logs')\n",
    "CHKPT_DIR = Path('chkpts')\n",
    "\n",
    "# create a directory for the TensorBoard logs\n",
    "if (os.path.exists(TB_LOG_DIR)):\n",
    "    shutil.rmtree(TB_LOG_DIR)\n",
    "os.makedirs(TB_LOG_DIR)\n",
    "print(\"Directory \" , TB_LOG_DIR ,  \"created \") \n",
    "\n",
    "# create a directory for the checkpoints\n",
    "if (os.path.exists(CHKPT_DIR)):\n",
    "    shutil.rmtree(CHKPT_DIR)\n",
    "os.makedirs(CHKPT_DIR)\n",
    "print(\"Directory \" , CHKPT_DIR ,  \"created \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Data wrangling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Next, we download the dataset. Keras conveniently provides the CIFAR-10 dataset and functions for loading it.\n",
    "\n",
    "The dataset is already split into training and test data - 50k images & labels for training, 10k images & labels for test.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The 'images' are actually numpy arrays, with a shape of (28,28) and datatype uint8. This means the actual data values of each element of the arrays (i.e. the pixels') can have a value of 0 to 255. Let's scale them back to range 0 to 1.0.  Note that dividing by 255.0 converts the array elements from integer to float.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For convenience, we'll create a list of labels for the 10 categories of image in the CIFAR-10 dataset. We 'll use it later when making predictions.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now for the training parameters. We'll set up the batch size to be 128, a learning rate of 0.0001 and decay rate of 1e-6 for the Adaptive Momentum optimizer.\n",
    "\n",
    "The maximum number of epochs is set to 250, but we are unlikely to reach this limit due to the Early Stop call back which we will see later.\n",
    "\n",
    "You are encourged to modify these parameters to see what effect they have on the final accuracy.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 128\n",
    "LEARN_RATE = 0.0001\n",
    "DECAY_RATE = 1e-6\n",
    "\n",
    "EPOCHS = 3\n",
    "#EPOCHS = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Define the sequential model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "This next section creates our CNN. It is a Keras Sequential model and built up of layers.\n",
    "\n",
    "Note how we need to define the shape of the input to the first layer, the others are automatically calculated.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Keras makes it easy to print out a summary of our network model...</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())\n",
    "print(\"Model Inputs: {ips}\".format(ips=(model.inputs)))\n",
    "print(\"Model Outputs: {ops}\".format(ops=(model.outputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Callbacks</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>..and now for the callbacks. These will be used during training.\n",
    "The first callback sets up the TensorBoard logging.\n",
    "The second one sets a limit for the training and will stop it if the loss doesn't improve by the value of min_delta (0.001 in this case) for at least 3 epochs.\n",
    "The third callback defines where the checkpoint will be saved.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensorboard callback\n",
    "tb_call = tf.keras.callbacks.TensorBoard(log_dir=TB_LOG_DIR,\n",
    "                                         histogram_freq=10,\n",
    "                                         batch_size=BATCHSIZE,\n",
    "                                         write_graph=True)\n",
    "\n",
    "# Early stop callback\n",
    "earlystop_call = tf.keras.callbacks.EarlyStopping(min_delta=0.001,\n",
    "                                                patience=3)\n",
    "\n",
    "# checkpoint save callback\n",
    "chk_call = tf.keras.callbacks.ModelCheckpoint(filepath=(os.path.join(CHKPT_DIR,'checkpoint.h5')), save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Training, evaluation, prediction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The .compile method defines the learning process by setting the type of optimizer (Adaptive Momentum in this case) and its parameters such as learning rate and decay rate and the metric that it needs to optimize.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the sequential model learning process\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=tf.keras.optimizers.Adam(lr=LEARN_RATE,\n",
    "                                                 decay=DECAY_RATE),\n",
    "                                                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The .fit method trains the model for a certain number of epochs.\n",
    "The validation data will be used to evaluate the model metrics at the end of each epoch.\n",
    "On both the training and test labels, we use the Keras .to_categorical() method to convert the scalar values to one-hot encoded vectors.\n",
    "Note that the callbacks we set up earlier are used here.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train,\n",
    "          y=tf.keras.utils.to_categorical(Y_train),\n",
    "          batch_size=BATCHSIZE,\n",
    "          shuffle=True,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=(X_test, tf.keras.utils.to_categorical(Y_test)),\n",
    "          callbacks=[earlystop_call,tb_call,chk_call])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The .evaluate method will use the supplied dataset to evaluate the trained model.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x=X_test,\n",
    "                        y=tf.keras.utils.to_categorical(Y_test))\n",
    "\n",
    "print('Loss: %.3f' % scores[0])\n",
    "print('Accuracy: %.3f' % scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>..and finally the .predict method will use the trained model to make some predictions - this would best be done using 'previously unseen' validation data, but here I'm just using the first 10 images from the test dataset.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLet's make some predictions with the trained model..\\n\")\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# each prediction is an array of 10 values\n",
    "# the max of the 10 values is the model's \n",
    "# highest \"confidence\" classification\n",
    "# use numpy argmax function to get highest of the set of 10\n",
    "\n",
    "for i in range(10):\n",
    "    pred=class_names[np.argmax(predictions[i])]\n",
    "    actual=class_names[(Y_test[i][0])]\n",
    "    print(\"Sample {index} in the test set is: {pred}\".format(index=i, pred=pred))\n",
    "    print(\"Sample {index} in test set actually is: {actual}\".format(index=i, actual=actual))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
