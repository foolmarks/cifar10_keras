{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue;\">CIFAR-10 example using Keras</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>This notebook will show you how to create, train and evaluate a small convolution network to work on the CIFAR-10 dataset.\n",
    "<br>\n",
    "The first thing we will do is the usual housekeeping ..import the required python libraries and set up directories. There's no machine learning specific code required here, just plain vanilla Python &#40;..I use Python3&#41;.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "from keras import datasets, utils, layers, models, optimizers, callbacks\n",
    "\n",
    "K_CHKPT_FILE = 'float-model-{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "K_MODEL_DIR = './k_model'\n",
    "K_CHKPT_DIR = './k_chkpts'\n",
    "TB_LOG_DIR = './tb_logs'\n",
    "K_CHKPT_PATH = os.path.join(K_CHKPT_DIR, K_CHKPT_FILE)\n",
    "\n",
    "\n",
    "# create a directory for the saved model if it doesn't already exist\n",
    "# delete it and recreate if it already exists\n",
    "if (os.path.exists(K_MODEL_DIR)):\n",
    "    shutil.rmtree(K_MODEL_DIR)\n",
    "os.makedirs(K_MODEL_DIR)\n",
    "print(\"Directory \" , K_MODEL_DIR ,  \"created \")\n",
    "\n",
    "\n",
    "# create a directory for the TensorBoard data if it doesn't already exist\n",
    "# delete it and recreate if it already exists\n",
    "if (os.path.exists(TB_LOG_DIR)):\n",
    "    shutil.rmtree(TB_LOG_DIR)\n",
    "os.makedirs(TB_LOG_DIR)\n",
    "print(\"Directory \" , TB_LOG_DIR ,  \"created \") \n",
    "\n",
    "\n",
    "# create a directory for the checkpoints if it doesn't already exist\n",
    "# delete it and recreate if it already exists\n",
    "if (os.path.exists(K_CHKPT_DIR)):\n",
    "    shutil.rmtree(K_CHKPT_DIR)\n",
    "os.makedirs(K_CHKPT_DIR)\n",
    "print(\"Directory \" , K_CHKPT_DIR ,  \"created \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Data wrangling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Next, we download the dataset. Keras conveniently provides the CIFAR-10 dataset and functions for loading it.\n",
    "\n",
    "The dataset is already split into training and test data - 50k images & labels for training, 10k images & labels for test.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The 'images' are actually numpy arrays, with a shape of (32,32) and datatype uint8. This means the actual data values of each element of the arrays (i.e. the pixels') can have a value of 0 to 255. Let's scale them back to range 0 to 1.0.  Note that dividing by 255.0 converts the array elements from integer to float.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For convenience, we'll create a list of labels for the 10 categories of image in the CIFAR-10 dataset. We 'll use it later when making predictions.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NExt we will 'steal' the last 5k images and labels from the training set to create a set of data which will remain unseen to the network during training and evaluation. We will use this new dataset to test the network after training has completed.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unseen dataset for predictions - 5k images\n",
    "X_predict = X_train[45000:]\n",
    "Y_predict = Y_train[45000:]\n",
    "\n",
    "# reduce training dataset to 45k images\n",
    "X_train = X_train[:45000]\n",
    "Y_train = Y_train[:45000]\n",
    "\n",
    "# one-hot encode the labels\n",
    "Y_train = utils.to_categorical(Y_train)\n",
    "Y_test = utils.to_categorical(Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now for the training parameters. We'll set up the batch size to be 128, a learning rate of 0.0001 and decay rate of 1e-6 for the Adaptive Momentum optimizer.\n",
    "\n",
    "The maximum number of epochs is set to 250, but we are unlikely to reach this limit due to the Early Stop call back which we will see later.\n",
    "\n",
    "You are encourged to modify these parameters to see what effect they have on the final accuracy.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 128\n",
    "LEARN_RATE = 0.0001\n",
    "DECAY_RATE = 1e-6\n",
    "\n",
    "EPOCHS = 1\n",
    "#EPOCHS = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Define the functional model on the CNN</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "This next section creates our CNN. It is a Keras functional model and built up of layers.\n",
    "\n",
    "Note how we need to define the shape of the input to the first layer, the others are automatically calculated.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miniVGGNet as Keras functional model\n",
    "\n",
    "inputs = layers.Input(shape=(32, 32, 3))\n",
    "net = layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu')(inputs)\n",
    "net = layers.BatchNormalization(axis=-1)(net)\n",
    "net = layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu')(net)\n",
    "net = layers.BatchNormalization(axis=-1)(net)\n",
    "net = layers.MaxPooling2D(pool_size=(2,2))(net)\n",
    "net = layers.Dropout(0.25)(net)\n",
    "net = layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu')(net)\n",
    "net = layers.BatchNormalization(axis=-1)(net)\n",
    "net = layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu')(net)\n",
    "net = layers.BatchNormalization(axis=-1)(net)\n",
    "net = layers.MaxPooling2D(pool_size=(2,2))(net)\n",
    "net = layers.Dropout(0.25)(net)\n",
    "net = layers.Flatten()(net)\n",
    "net = layers.Dense(512, activation='relu')(net)\n",
    "net = layers.BatchNormalization()(net)\n",
    "net = layers.Dropout(0.5)(net)\n",
    "prediction = layers.Dense(10, activation='softmax')(net)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Keras makes it easy to print out a summary of our network model...</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())\n",
    "print(\"Model Inputs: {ips}\".format(ips=(model.inputs)))\n",
    "print(\"Model Outputs: {ops}\".format(ops=(model.outputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Callbacks</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>..and now for the callbacks. These will be used during training.\n",
    "The first callback sets up the TensorBoard logging.\n",
    "The second one sets a limit for the training and will stop it if the loss doesn't improve by the value of min_delta (0.001 in this case) for at least 3 epochs.\n",
    "The third callback defines where the checkpoint will be saved.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensorboard callback\n",
    "tb_call = callbacks.TensorBoard(log_dir=TB_LOG_DIR,\n",
    "                                         histogram_freq=10,\n",
    "                                         batch_size=BATCHSIZE,\n",
    "                                         write_graph=True,\n",
    "                                         write_grads=False,\n",
    "                                         write_images=False )\n",
    "\n",
    "\n",
    "# Early stop callback\n",
    "earlystop_call = callbacks.EarlyStopping(min_delta=0.001, patience=3)\n",
    "\n",
    "# checkpoint save callback\n",
    "chk_call = callbacks.ModelCheckpoint(K_CHKPT_PATH, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Training, evaluation, prediction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The .compile method defines the learning process by setting the type of optimizer (Adaptive Momentum in this case) and its parameters such as learning rate and decay rate and the metric that it needs to optimize.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizers.Adam(lr=LEARN_RATE, decay=DECAY_RATE),\n",
    "              metrics=['accuracy']\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The .fit method trains the model for a certain number of epochs.\n",
    "The validation data will be used to evaluate the model metrics at the end of each epoch.\n",
    "On both the training and test labels, we use the Keras .to_categorical() method to convert the scalar values to one-hot encoded vectors.\n",
    "Note that the callbacks we set up earlier are used here.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=BATCHSIZE,\n",
    "          shuffle=True,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[earlystop_call,tb_call,chk_call])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The .evaluate method will use the supplied dataset to evaluate the trained model.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, \n",
    "                        Y_test,\n",
    "                        batch_size=BATCHSIZE\n",
    "                        )\n",
    "\n",
    "print('Loss: %.3f' % scores[0])\n",
    "print('Accuracy: %.3f' % scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>..and then the .predict method will use the trained model to make some predictions - this would best be done using 'previously unseen' validation data, but here I'm just using the first 10 images from the test dataset.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Make some predictions with the trained model..')\n",
    "predictions = model.predict(X_predict, batch_size=BATCHSIZE)\n",
    "\n",
    "# each prediction is an array of 10 values\n",
    "# the max of the 10 values is the model's \n",
    "# highest \"confidence\" classification\n",
    "# use numpy argmax function to get highest of the set of 10\n",
    "\n",
    "correct_predictions = 0\n",
    "wrong_predictions = 0\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    pred=np.argmax(predictions[i])\n",
    "    actual=(Y_predict[i][0])\n",
    "\n",
    "    if (pred == actual):\n",
    "        correct_predictions += 1\n",
    "    else:\n",
    "        wrong_predictions += 1\n",
    "\n",
    "print('Validation dataset size: ' , len(predictions), ' Correct Predictions: ', correct_predictions, ' Wrong Predictions: ', wrong_predictions)\n",
    "print ('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving the Keras model in keras format..\")\n",
    "\n",
    "# save just the weights (no architecture) to an HDF5 format file\n",
    "model.save_weights(os.path.join(K_MODEL_DIR,'k_model_weights.h5'))\n",
    "\n",
    "# save just the architecture (no weights) to a JSON file\n",
    "with open(os.path.join(K_MODEL_DIR,'k_model_architecture.json'), 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "\n",
    "# save weights, model architecture & optimizer to an HDF5 format file\n",
    "model.save(os.path.join(K_MODEL_DIR,'k_complete_model.h5'))\n",
    "\n",
    "\n",
    "print('FINISHED!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
